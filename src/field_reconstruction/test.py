

import torch
import os
import numpy as np
from tqdm import tqdm
from models import FukamiNet, ReconstructionVAE
import matplotlib.pyplot as plt
from utils import get_device
from plots_creator import plot_voronoi_reconstruction_comparison, plot_l2_error_distributions
from utils import create_model, get_device
from scipy.interpolate import griddata
from skimage.metrics import structural_similarity as ssim

def rrmse(pred, target):
    """
    The function calculates the relative root mean square error between two input tensors.
    
    :param pred: The `pred` parameter typically refers to the predicted values from a model, while the
    `target` parameter refers to the actual target values. The function `rrmse` seems to be calculating
    the Root Relative Mean Squared Error (RRMSE) between the predicted values and the target values
    :param target: The `target` parameter typically refers to the ground truth values or the actual
    values that you are trying to predict or model. In the context of the `rrmse` function you provided,
    `target` is likely the true target values that you are comparing against the predicted values
    (`pred`)
    :return: the Relative Root Mean Squared Error (RRMSE) between the predicted values (pred) and the
    target values.
    """
    
    return torch.sqrt(torch.mean((pred - target) ** 2)) / torch.sqrt(torch.mean(target ** 2))

def mae(pred, target):
    """
    The function calculates the mean absolute error between two input tensors.
    
    :param pred: The `pred` parameter typically refers to the predicted values generated by a model,
    while the `target` parameter refers to the actual target values that the model is trying to predict.
    The `mae` function calculates the Mean Absolute Error (MAE) between the predicted values (`pred`)
    and the
    :param target: The `target` parameter typically refers to the true or actual values that you are
    trying to predict or estimate in a machine learning model. It is the ground truth against which your
    predictions are compared
    :return: The function `mae` returns the mean absolute error between the `pred` and `target` tensors
    using the torch library.
    """
    return torch.mean(torch.abs(pred - target))

def evaluate_interp(test_loader, mode:int=3):
    device = get_device()
    
    print(f"âœ… Evaluating model on {len(test_loader.dataset)} samples")
    print(f"âœ… Model type: Interpolation (order {mode})")
    print(f"âœ… Device: {device}")
    
    rrmse_total = 0.0
    mae_total = 0.0
    n = 0
    
    with torch.no_grad():
        pbar = tqdm(test_loader, desc="Testing")
        for inputs, targets in pbar:
            inputs, targets = inputs.to(device), targets.to(device)
            
            inputs_np = inputs.cpu().numpy()
            targets_np = targets.cpu().numpy()
            
            # Get voronoi-tessellated values and mask
            tess = inputs_np[:, 0, :, :]  # batch x H x W
            mask = inputs_np[:, 1, :, :]  # batch x H x W
            
            preds = []
            for i in range(tess.shape[0]):
                H, W = tess[i].shape
                yx = np.argwhere(mask[i] > 0)
                values = tess[i][mask[i] > 0]
                
                grid_y, grid_x = np.meshgrid(np.arange(H), np.arange(W), indexing="ij")
                interp = griddata(yx, values, (grid_y, grid_x), method='cubic', fill_value=0.0)
                preds.append(torch.tensor(interp, dtype=torch.float32))
            
            preds = torch.stack(preds).unsqueeze(1).to(device)
            rrmse_batch = rrmse(preds, targets).item()
            mae_batch = mae(preds, targets).item()
            
            rrmse_total += rrmse_batch * inputs.size(0)
            mae_total += mae_batch * inputs.size(0)
            n += inputs.size(0)
            
            pbar.set_postfix({"RRMSE": rrmse_batch, "MAE": mae_batch})

    rrmse_avg = rrmse_total / n
    mae_avg = mae_total / n
    print(f"âœ… Interpolation Test RRMSE: {rrmse_avg:.6f}")
    print(f"âœ… Interpolation Test MAE:   {mae_avg:.6f}")

    return {"rrmse": rrmse_avg, "mae": mae_avg}

def evaluate(model_type, test_loader, checkpoint_path, variable_names=None):
    from pathlib import Path  # <-- make sure this is imported
    device = get_device()

    if model_type == "cubic_interpolation":
        evaluate_interp(test_loader, mode=3)
        return

    sample_input, _ = next(iter(test_loader))
    nb_channels = sample_input.shape[1]
    model = create_model(model_type, nb_channels=nb_channels).to(device)

    checkpoint_path = os.path.join("models/saves", checkpoint_path)
    model.load_state_dict(torch.load(checkpoint_path, map_location=device))
    model.eval()
    print(f"âœ… Loaded model from {checkpoint_path}")
    print(f"âœ… Evaluating model on {len(test_loader.dataset)} samples")
    print(f"âœ… Model type: {model_type}")
    print(f"âœ… Device: {device}")

    rrmse_total, mae_total, ssim_total = [], [], []
    l2_errors = [[] for _ in range(nb_channels)]
    n_total = 0

    with torch.no_grad():
        pbar = tqdm(test_loader, desc="Testing")
        for inputs, targets in pbar:
            inputs, targets = inputs.to(device), targets.to(device)

            if model_type == "vae":
                recon_x, mu, logvar = model(inputs)
                preds = recon_x
            else:
                preds = model(inputs)

            batch_size, nb_channels, H, W = targets.shape
            rrmse_batch, mae_batch, ssim_batch = [], [], []

            preds_np = preds.detach().cpu().numpy()
            targets_np = targets.detach().cpu().numpy()

            for v in range(nb_channels):
                preds_v = preds_np[:, v, :, :]
                targets_v = targets_np[:, v, :, :]

                rrmse_val = np.sqrt(np.mean((preds_v - targets_v) ** 2)) / (np.sqrt(np.mean(targets_v ** 2)) + 1e-8)
                mae_val = np.mean(np.abs(preds_v - targets_v))
                ssim_val = np.mean([
                    ssim(preds_v[i], targets_v[i], data_range=targets_v[i].max() - targets_v[i].min() + 1e-8)
                    for i in range(preds_v.shape[0])
                ])
                l2_vals = np.mean((preds_v - targets_v) ** 2, axis=(1, 2))
                l2_errors[v].extend(l2_vals.tolist())

                rrmse_batch.append(rrmse_val)
                mae_batch.append(mae_val)
                ssim_batch.append(ssim_val)

            rrmse_total.append(np.array(rrmse_batch) * batch_size)
            mae_total.append(np.array(mae_batch) * batch_size)
            ssim_total.append(np.array(ssim_batch) * batch_size)
            n_total += batch_size

            pbar.set_postfix({
                "RRMSE": np.mean(rrmse_batch),
                "MAE": np.mean(mae_batch),
                "SSIM": np.mean(ssim_batch)
            })

    rrmse_total = np.sum(rrmse_total, axis=0) / n_total
    mae_total = np.sum(mae_total, axis=0) / n_total
    ssim_total = np.sum(ssim_total, axis=0) / n_total

    if variable_names is None:
        variable_names = [f"Var{i}" for i in range(len(rrmse_total))]

    print("ðŸ“ˆ Per-variable Metrics:")
    for idx, var in enumerate(variable_names):
        print(f"âœ… {var}: RRMSE={rrmse_total[idx]:.4f}, MAE={mae_total[idx]:.4f}, SSIM={ssim_total[idx]:.4f}")

    print("\nðŸ“Š Overall Averages:")
    print(f"âœ… Avg RRMSE={np.mean(rrmse_total):.4f}, Avg MAE={np.mean(mae_total):.4f}, Avg SSIM={np.mean(ssim_total):.4f}")

    # Convert to dict
    l2_errors_dict = {var: l2_errors[i] for i, var in enumerate(variable_names)}

    # ðŸ“Š Plot L2 Error Distributions
    save_dir = Path("plots/evaluation")
    save_dir.mkdir(parents=True, exist_ok=True)
    plot_l2_error_distributions(l2_errors_dict, variable_names, model_type, str(save_dir),)

    return {
        "rrmse": np.mean(rrmse_total),
        "mae": np.mean(mae_total),
        "ssim": np.mean(ssim_total),
        "rrmse_per_var": rrmse_total,
        "mae_per_var": mae_total,
        "ssim_per_var": ssim_total,
        "l2_per_var": l2_errors,
    }