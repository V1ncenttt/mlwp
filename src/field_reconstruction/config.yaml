# config.yaml

project: mlwp-thesis-fieldreco
entity: your-wandb-username  # replace with your wandb user/org name


model:
  type: transformer           # or cnn, diffusion, etc.
  hidden_dim: 256
  num_layers: 6
  dropout: 0.1
  input_channels: 3          # e.g., T2M, U10, V10
  output_channels: 1         # e.g., T2M forecast

data:
  dataset: weatherbench2
  root_dir: /path/to/data
  resolution: 5.625deg
  variables: [t2m, u10, v10]

train:
  epochs: 50
  batch_size: 32
  learning_rate: 0.0003
  optimizer: adamw
  scheduler: cosine
  weight_decay: 1e-4

loss: mse

logging:
  log_every: 10
  save_every: 1
  use_wandb: true

split_ratio: 0.8
model: fukami_unet
percent: 10
batch_size: 32
variable: 2m_temperature
output_dir: ../../data/weatherbench2_fieldreco/
optimizer: adam
learning_rate: 0.001
weight_decay: 0.01
epochs: 10
tags: [baseline, transformer, lowres]
notes: Initial experiment using Transformer on WB2 5.625Â°
test:
  test_dataset: 
  model_path: vae_model_10_0.121212.pth
  model_type: vae
wandb:
  project: mlwp-thesis-fieldreco
  entity: your-wandb-username  # replace with your wandb user/org name
  log_model: true
  log_data: true
  log_config: true
  log_metrics: true
  log_artifacts: true
  log_code: true
  log_hyperparameters: true
  log_git: true
  log_env: true
  log_system: true
  log_tensorboard: false
  log_mlflow: false
